{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id, do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(model_id)\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tokenizer can represent any word by spelling it, if necessary\n",
    "all('##'+c in tokenizer.vocab for c in 'abcdefghijklmnopqrstuvwxyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', '##k', '##cd']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('xkcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['foods', '##er', '##vic', '##e'], ['food', 'service'])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but it's not all that smart\n",
    "tokenizer.tokenize('foodservice'), tokenizer.tokenize('food service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and vocab is missing many proper nouns\n",
    "# (cased model will spell this from parts, uncased will return '[UNK]' because of the capital 'A')\n",
    "tokenizer.tokenize('Alcatraz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['firearms', 'irritated', 'highest', 'adoptive', 'comedians',\n",
       "       'coached', 'seeking', 'shin', 'recruits', 'fighters', 'ultrasound',\n",
       "       '##arium', '##ul', 'pose', 'shut', 'nj', 'loops', 'figure',\n",
       "       '##moor', 'orton', '##yev', 'replace', 'grade', 'ears',\n",
       "       '[unused337]', '##ry', '##oja', 'soluble', '##omorphic',\n",
       "       'sectional', 'territories', 'consultant', 'deported', 'little',\n",
       "       'hungary', 'immunity', 'hines', 'majority', 'gradual', '##ph',\n",
       "       'koppen', 'munich', 'diseases', 'violence', 'descendant',\n",
       "       'custody', '##oi', 'freight', '##ン', '▪', 'usable', '##rwood',\n",
       "       'questioning', 'stabbed', 'ports', 'fear', 'scanned', 'dialogues',\n",
       "       'nail', 'artwork', '##力', 'ramsey', '##oy', '##ister', 'pneumonia',\n",
       "       'roadside', 'mrna', 'piedmont', 'bullying', 'madden', 'journeys',\n",
       "       'confident', 'superintendent', 'darker', 'rake', 'scroll', '##そ',\n",
       "       'conservatoire', '##phi', 'obscene', 'skopje', 'liberty',\n",
       "       'specialization', 'staggering', '##tics', 'inline',\n",
       "       'controversies', 'cremated', 'loving', 'guadalupe', '##schaft',\n",
       "       'greyhound', 'mistakenly', 'inventor', 'staged', 'miles',\n",
       "       'compression', 'raw', 'rigorous', 'deposed', 'given', 'toys',\n",
       "       '##sius', '##ame', '126', 'cocked', 'allergic', '[unused158]',\n",
       "       'indicates', 'laurence', '##،', 'waterloo', 'channel',\n",
       "       'communicating', 'defensive', 'unbelievable', '##π', '##kney',\n",
       "       'saxe', '##36', 'delivered', '##ged', 'nassau', 'स', 'restricted',\n",
       "       '##eet', 'vantage', 'fitness', '337', 'novels', '490', 'opted',\n",
       "       'shepard', 'betting', '皇', 'modernized', 'fraternity', 'mendoza',\n",
       "       'burning', '##ily', 'welfare', 'versailles', 'catalogue', '##cd',\n",
       "       '##mus', '##witz', 'crossover', '##tness', 'threaten',\n",
       "       'automobiles', 'done', 'tolerance', 'specially', '##ே', 'pirate',\n",
       "       'disability', 'grabbing', '##cen', '##free', '1905', 'about',\n",
       "       'exponential', '##lino', 'dare', 'broader', 'portland', 'ranged',\n",
       "       '‿', '##lch', 'medina', 'greeted', 'prove', 'coloring', 'mondays',\n",
       "       'periodic', 'abducted', 'queensland', 'ashford', 'ashton', 'prix',\n",
       "       '2nd', '##blood', 'ethan', '##lists', 'southernmost', 'delegate',\n",
       "       'kill', '##thorpe', '##ctor', 'shock', 'endurance', 'shenzhen',\n",
       "       'texture', 'decide', 'team', 'undo', 'joining', 'null', '##ider',\n",
       "       '1982'], dtype='<U18')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(list(tokenizer.vocab.keys()), size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = tokenizer.tokenize('this is a simple test sentence.')\n",
    "# text = tokenizer.tokenize('certain methods handle self-referential sentences poorly.')\n",
    "# text = tokenizer.tokenize('who was jim henson?')\n",
    "# text = tokenizer.tokenize('Is there life on Mars?')\n",
    "# text = tokenizer.tokenize('The Occupation of Alcatraz was an occupation of Alcatraz Island by 89 American Indians and supporters, led by Richard Oakes, LaNada Means, and others.')\n",
    "text = tokenizer.tokenize('beam search uses breadth-first search to build its search tree.')\n",
    "# response = tokenizer.tokenize('jim henson was a puppeteer')\n",
    "response = ['[MASK]']*5 + ['!']\n",
    "# response = list(np.random.choice([t for t in tokenizer.vocab if not t.startswith('[unused')], size=5)) + ['.']\n",
    "# response = tokenizer.tokenize('so is this one.')\n",
    "# response = tokenizer.tokenize('however, some may perform better than others.')\n",
    "# response = tokenizer.tokenize('They chose the name Indians of All Tribes and John Trudell was the spokesperson.')\n",
    "# response = tokenizer.tokenize('At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost')\n",
    "# response = ['[MASK]']*(len(response)-1) + ['.']\n",
    "# response[0] = '[MASK]'\n",
    "# response[3] = '[MASK]'\n",
    "# response[5] = '[MASK]'\n",
    "response[3] = 'computer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = torch.tensor(tokenizer.convert_tokens_to_ids(text+response))\n",
    "# segments = torch.tensor([0]*len(text) + [1]*len(response))\n",
    "tokens = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]']+text+['[SEP]']+response))\n",
    "segments = torch.tensor([0]*(len(text)+2) + [1]*len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_prior = torch.tensor([1.0]*len(tokenizer.vocab))\n",
    "# token_prior[1012] *= 0\n",
    "# token_prior[[1000, 1006, 1007, 1010]] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] beam search uses breadth - first search to build its search tree . [SEP] [MASK] [MASK] [MASK] computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(' '.join(tokenizer.convert_ids_to_tokens(np.array(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_text(t):\n",
    "    display(' '.join(tokenizer.convert_ids_to_tokens(np.array(t))).replace(' ##', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK] [MASK] [MASK] computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[MASK] : [MASK] computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : [MASK] computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : hello computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# deterministically fix the most confident [MASK] token one at a time (resembles beam search with size 1?)\n",
    "pred_tokens = tokens.clone()\n",
    "idxs_to_predict = [i for i,t in enumerate(tokens) if t==103]\n",
    "while len(idxs_to_predict):    \n",
    "    pred = (\n",
    "        model(pred_tokens.unsqueeze(dim=0), segments.unsqueeze(dim=0))[0]\n",
    "        .index_select(0, torch.LongTensor(idxs_to_predict))\n",
    "        )\n",
    "    pred = F.softmax(pred, -1)*token_prior\n",
    "    \n",
    "    p, i = torch.max(pred, dim=-1)\n",
    "    most_confident_idx = torch.argmax(p).item()\n",
    "    token_id = i[most_confident_idx].item()\n",
    "    pred_tokens[idxs_to_predict[most_confident_idx]] = token_id\n",
    "    \n",
    "    display_text(pred_tokens[len(text)+2:])\n",
    "    del idxs_to_predict[most_confident_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'otherwise : from computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'otherwise : from ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = ! ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + + !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + + !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + + !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + + !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + + +'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + + +'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'example : = + + +'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iteratively sample from naive joint distribution \n",
    "# does not resemble data distribution but generates pretty concrete poetry\n",
    "temp=1\n",
    "pred_tokens = tokens.clone()\n",
    "for _ in range(32):\n",
    "    pred = model(pred_tokens.unsqueeze(dim=0), segments.unsqueeze(dim=0))[0, len(text)+2:]\n",
    "    pred = F.softmax(pred, -1).pow(1/temp)\n",
    "    pred = pred*token_prior\n",
    "    pred_tokens[len(text)+2:] = pred.multinomial(1).flatten()\n",
    "    display_text(pred_tokens[len(text)+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mark [MASK] [MASK] computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mark goodnight [MASK] computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mark goodnight from computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mark goodnight from computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mark goodnight from computer help !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mark goodnight from computer help !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iteratively sample tokens from softmax, left to right\n",
    "temp = 1\n",
    "pred_tokens = tokens.clone()\n",
    "for i in range(len(text)+2, len(tokens)):\n",
    "    pred = model(pred_tokens.unsqueeze(dim=0), segments.unsqueeze(dim=0))[0, i]\n",
    "    pred = F.softmax(pred, -1)\n",
    "    pred = pred.pow(1/temp)\n",
    "    pred_tokens[i] = pred.multinomial(1).item()\n",
    "    display_text(pred_tokens[len(text)+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they [MASK] [MASK] computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'they watch [MASK] computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'they watch their computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'they watch their computer ! !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # iteratively sample from second sentence, one random position at a time\n",
    "temp = 1\n",
    "pred_tokens = tokens.clone()\n",
    "idxs_to_predict = [i for i,t in enumerate(tokens) if t==103]\n",
    "while 103 in pred_tokens:\n",
    "    pred = (\n",
    "        model(pred_tokens.unsqueeze(dim=0), segments.unsqueeze(dim=0))[0]\n",
    "        .index_select(0, torch.LongTensor(idxs_to_predict))\n",
    "    )\n",
    "    i = np.random.choice(len(idxs_to_predict))\n",
    "    pred_tokens[idxs_to_predict[i]] = F.softmax(pred[i], -1).pow(1/temp).multinomial(1).item()\n",
    "    display_text(pred_tokens[len(text)+2:])\n",
    "    del idxs_to_predict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stars [MASK] [MASK] computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stars [MASK] [MASK] arcade [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stars [MASK] of arcade [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stars appropriated of arcade [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stars reminiscent of arcade [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stars reminiscent of arcade fire !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iteratively sample one position at a time, but choose i to have maximum entropy\n",
    "# usually terminates pretty fast but can occasionally get stuck,\n",
    "# if there's a lower entropy dist. over a chosen token than any mask tokens.\n",
    "temp = 1\n",
    "pred_tokens = tokens.clone()\n",
    "k = len(text)+2\n",
    "it=0\n",
    "while 103 in pred_tokens and it < 64:# or it < 2*len(response) :\n",
    "    it+=1\n",
    "    pred = model(pred_tokens.unsqueeze(dim=0), segments.unsqueeze(dim=0))[0, k:]\n",
    "    pred = F.softmax(pred, dim=-1)\n",
    "    ent = -(pred.log() * pred).sum(dim=-1)\n",
    "#     display(ent)\n",
    "    i = ent.argmax()\n",
    "    pred_tokens[i+k] = pred[i].pow(1/temp).multinomial(1).item()\n",
    "    display_text(pred_tokens[len(text)+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK] [MASK] - computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[MASK] facebook - computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[MASK] facebook - computer search !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'facebook facebook - computer search !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iteratively sample one mask position at a time, but choose i to have minimum entropy\n",
    "temp = 1\n",
    "pred_tokens = tokens.clone()\n",
    "idxs_to_predict = [i for i,t in enumerate(tokens) if t==103]\n",
    "while 103 in pred_tokens:\n",
    "    pred = (\n",
    "        model(pred_tokens.unsqueeze(dim=0), segments.unsqueeze(dim=0))[0]\n",
    "        .index_select(0, torch.LongTensor(idxs_to_predict))\n",
    "    )\n",
    "    pred = F.softmax(pred, dim=-1)\n",
    "    ent = -(pred.log() * pred).sum(dim=-1)\n",
    "#     display(ent)\n",
    "    i = ent.argmin()\n",
    "    \n",
    "    pred_tokens[idxs_to_predict[i]] = pred[i].pow(1/temp).multinomial(1).item()\n",
    "    display_text(pred_tokens[len(text)+2:])\n",
    "    del idxs_to_predict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK] [MASK] [MASK] computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[MASK] [MASK] means computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[MASK] h means computer [MASK] !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[MASK] h means computer search !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'field h means computer search !'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iteratively fix one [MASK] token at a time, treating the flattened logits as one big softmax\n",
    "temp = 1\n",
    "pred_tokens = tokens.clone()\n",
    "idxs_to_predict = [i for i,t in enumerate(tokens) if t==103]\n",
    "# idxs_to_predict = list(range(2+len(text), len(tokens)))\n",
    "while True:\n",
    "    display_text(pred_tokens[len(text)+2:])\n",
    "    \n",
    "    if not len(idxs_to_predict): break\n",
    "    \n",
    "    pred = (\n",
    "        model(pred_tokens.unsqueeze(dim=0), segments.unsqueeze(dim=0))[0]\n",
    "        .index_select(0, torch.LongTensor(idxs_to_predict))\n",
    "        )\n",
    "    pred = (pred.exp()*token_prior).view(-1)\n",
    "    if temp!=1:\n",
    "        pred = pred/pred.sum()\n",
    "        pred = pred.pow(1/temp)\n",
    "    \n",
    "    ij = pred.multinomial(1).item()\n",
    "    i = ij//len(tokenizer.vocab)\n",
    "    j = ij%len(tokenizer.vocab)\n",
    "    pred_tokens[idxs_to_predict[i]] = j\n",
    "    del idxs_to_predict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea: linguistic erosion stalactites. iteratively delete, replace, shorten tokens\n",
    "# for max entropy position, zero prob of current token then sample\n",
    "# delete min entropy position\n",
    "# zero prob of all tokens longer than current token in position, then sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
